# Archon 72 Environment Configuration
# Copy this file to .env and adjust values as needed

# Database (PostgreSQL via Docker)
DATABASE_URL=postgresql://postgres:postgres@localhost:54322/archon72

# Redis
REDIS_URL=redis://localhost:6379

# Development mode (enables dev-only features and watermarks)
DEV_MODE=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Ollama (Local LLM Server)
# Used when archon config has provider: local
# Set to your Ollama server address
OLLAMA_HOST=http://localhost:11434

# Ollama Cloud (optional)
# Use OLLAMA_BASE_URL + OLLAMA_API_KEY for cloud inference
# OLLAMA_BASE_URL=https://ollama.com
# OLLAMA_API_KEY=your_api_key_here
# OLLAMA_CLOUD_ENABLED=true

# Ollama Cloud rate limiting (validator workers)
# OLLAMA_MAX_CONCURRENT=5
# OLLAMA_RETRY_MAX_ATTEMPTS=5
# OLLAMA_RETRY_BASE_DELAY=1.0
# OLLAMA_RETRY_MAX_DELAY=60.0

# Cloud Providers (uncomment and set as needed)
# ANTHROPIC_API_KEY=sk-ant-...
# OPENAI_API_KEY=sk-...

# =============================================================================
# ROLE-BASED ARCHON OVERRIDES
# =============================================================================
# Bind specific scripts to a specific Archon profile for LLM configuration.
# These should be Archon UUIDs from docs/archons-base.json.
EXECUTION_PLANNER_ARCHON_ID=
CONSOLIDATOR_ARCHON_ID=

# =============================================================================
# CONCURRENCY SETTINGS
# =============================================================================
# Defaults for LLM-heavy stages (tune based on provider limits)
REVIEWER_ARCHON_CONCURRENCY=4
REVIEWER_PANEL_CONCURRENCY=2

# =============================================================================
# 3-ARCHON VOTE VALIDATION PROTOCOL
# =============================================================================
# Constitutional separation of powers for vote interpretation:
#
# DETERMINATION (2 Secretaries must agree):
#   - SECRETARY_TEXT: Interprets vote using natural language analysis
#   - SECRETARY_JSON: Interprets vote using structured output analysis
#   - Both must reach consensus on AYE/NAY/ABSTAIN
#   - Disagreement triggers retry (up to VOTE_VALIDATION_MAX_ATTEMPTS)
#
# WITNESS (1 Knight-Witness observes):
#   - Reviews secretaries' determination
#   - AGREES: Records acknowledgment on the record
#   - DISSENTS: Records objection (CANNOT change outcome)
#   - Every vote has explicit witness statement for audit trail
#
# Flow: Vote → [Secretary Text + Secretary JSON] → Consensus? → Witness → Validated

# Secretaries: Determine vote choice (must reach consensus)
SECRETARY_TEXT_ARCHON_ID=43d83b84-243b-49ae-9ff4-c3f510db9982
SECRETARY_JSON_ARCHON_ID=71f9ad05-acb2-46d8-a391-88d86ac55ec8

# Witness: Observes and records (cannot change outcome)
WITNESS_ARCHON_ID=1b872789-7990-4163-b54b-6bc45746e2f6

# Max retry attempts on secretary disagreement (ADR-005)
# After this many attempts without consensus, vote routes to DLQ
VOTE_VALIDATION_MAX_ATTEMPTS=3

# =============================================================================
# ASYNC VOTE VALIDATION (Kafka)
# =============================================================================
# Enable async vote validation via Kafka (ADR-001)
# When false or Kafka unhealthy, falls back to synchronous validation
ENABLE_ASYNC_VALIDATION=false

# Kafka audit trail (in-process async validation)
# When enabled, audit events are published to Kafka topics
KAFKA_ENABLED=false
KAFKA_TOPIC_PREFIX=conclave

# Kafka/Redpanda connection (required when async validation enabled)
KAFKA_BOOTSTRAP_SERVERS=localhost:19092
SCHEMA_REGISTRY_URL=http://localhost:18081

# Consumer group for validator workers
KAFKA_CONSUMER_GROUP=conclave-validators

# Reconciliation settings
# Seconds to wait for all validations at session adjournment (P1, P2)
# (alias: RECONCILIATION_TIMEOUT_SECONDS)
VOTE_VALIDATION_TIMEOUT=300
# New alias for in-process validation
# RECONCILIATION_TIMEOUT=300

# Agent invocation timeouts (debate/vote/validation)
# Retries only on timeout (other errors propagate as-is)
# AGENT_TIMEOUT_SECONDS=180
# AGENT_TIMEOUT_MAX_ATTEMPTS=3
# AGENT_TIMEOUT_BASE_DELAY_SECONDS=2.0
# AGENT_TIMEOUT_MAX_DELAY_SECONDS=30.0

# Circuit breaker settings (Story 2.2.1)
# Failures before circuit opens
CIRCUIT_BREAKER_FAILURE_THRESHOLD=3
# Seconds before half-open state
CIRCUIT_BREAKER_RESET_TIMEOUT=30

# =============================================================================
# DELIBERATION MODE
# =============================================================================
# PARALLEL:   All 72 agents deliberate concurrently (requires multi-GPU fleet)
# SEQUENTIAL: Agents deliberate one at a time (single GPU, accuracy-focused)
#
# Sequential mode is recommended for:
# - Testing and development
# - Single-GPU deployments
# - Accuracy over speed scenarios
# - Model swapping between different model sizes
DELIBERATION_MODE=sequential

# =============================================================================
# SINGLE-GPU SEQUENTIAL SETUP (Recommended for Testing)
# =============================================================================
# For testing on a single GPU with quantized models:
#
# 1. Set OLLAMA_HOST to your Ollama server:
#    OLLAMA_HOST=http://192.168.1.66:11434
#
# 2. Pull quantized models on your Ollama server:
#    ollama pull gpt-oss:20b-q4_K_M    # Kings (~12GB VRAM)
#    ollama pull qwen3:8b-q4_K_M       # Dukes (~5GB VRAM)
#    ollama pull llama3.2:8b-q4_K_M    # Directors (~5GB VRAM)
#    ollama pull gemma3:4b-q4_K_M      # Strategic (~3GB VRAM)
#
# 3. Update config/archon-llm-bindings.yaml to use quantized model names
#
# 4. Set DELIBERATION_MODE=sequential above
#
# Estimated deliberation time for 72 agents (sequential):
#   - Fast GPU (H100/A100): ~15-30 minutes
#   - Medium GPU (RTX 4090/3090): ~30-60 minutes
#   - Slower GPU: ~1-2 hours
#
# Production upgrade path:
#   - Single A100/H100 80GB can run all models at FP16 (no quantization)
#   - Multi-GPU fleet enables parallel mode for faster deliberation
