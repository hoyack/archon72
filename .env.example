# Archon 72 Environment Configuration
# Copy this file to .env and adjust values as needed

# Database (PostgreSQL via Docker)
DATABASE_URL=postgresql://postgres:postgres@localhost:54322/archon72

# Redis
REDIS_URL=redis://localhost:6379

# Development mode (enables dev-only features and watermarks)
DEV_MODE=true

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Ollama (Local LLM Server)
# Used when archon config has provider: local
# Set to your Ollama server address
OLLAMA_HOST=http://localhost:11434

# Cloud Providers (uncomment and set as needed)
# ANTHROPIC_API_KEY=sk-ant-...
# OPENAI_API_KEY=sk-...

# =============================================================================
# ROLE-BASED ARCHON OVERRIDES
# =============================================================================
# Bind specific scripts to a specific Archon profile for LLM configuration.
# These should be Archon UUIDs from docs/archons-base.json.
EXECUTION_PLANNER_ARCHON_ID=
SECRETARY_TEXT_ARCHON_ID=
SECRETARY_JSON_ARCHON_ID=
CONSOLIDATOR_ARCHON_ID=

# =============================================================================
# DELIBERATION MODE
# =============================================================================
# PARALLEL:   All 72 agents deliberate concurrently (requires multi-GPU fleet)
# SEQUENTIAL: Agents deliberate one at a time (single GPU, accuracy-focused)
#
# Sequential mode is recommended for:
# - Testing and development
# - Single-GPU deployments
# - Accuracy over speed scenarios
# - Model swapping between different model sizes
DELIBERATION_MODE=sequential

# =============================================================================
# SINGLE-GPU SEQUENTIAL SETUP (Recommended for Testing)
# =============================================================================
# For testing on a single GPU with quantized models:
#
# 1. Set OLLAMA_HOST to your Ollama server:
#    OLLAMA_HOST=http://192.168.1.66:11434
#
# 2. Pull quantized models on your Ollama server:
#    ollama pull gpt-oss:20b-q4_K_M    # Kings (~12GB VRAM)
#    ollama pull qwen3:8b-q4_K_M       # Dukes (~5GB VRAM)
#    ollama pull llama3.2:8b-q4_K_M    # Directors (~5GB VRAM)
#    ollama pull gemma3:4b-q4_K_M      # Strategic (~3GB VRAM)
#
# 3. Update config/archon-llm-bindings.yaml to use quantized model names
#
# 4. Set DELIBERATION_MODE=sequential above
#
# Estimated deliberation time for 72 agents (sequential):
#   - Fast GPU (H100/A100): ~15-30 minutes
#   - Medium GPU (RTX 4090/3090): ~30-60 minutes
#   - Slower GPU: ~1-2 hours
#
# Production upgrade path:
#   - Single A100/H100 80GB can run all models at FP16 (no quantization)
#   - Multi-GPU fleet enables parallel mode for faster deliberation
