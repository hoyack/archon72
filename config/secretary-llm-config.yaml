# Secretary Agent LLM Configuration
# =================================
# The Conclave Secretary is the 73rd agent - a special administrative agent
# that processes transcripts rather than participating in deliberations.
#
# The Secretary uses a dual-model approach:
# - Text model: For natural language extraction and analysis
# - JSON model: For structured output formatting (clustering, motions)
#
# Default configuration uses local Ollama - set OLLAMA_HOST env var.

secretary:
  # Reserved ID for the Secretary (73rd agent)
  id: "00000000-0000-0000-0000-000000000073"

  # Text Processing Model (extraction, analysis)
  # Good at understanding nuanced language and extracting meaning
  text_model:
    provider: local
    model: ministral-3:latest
    temperature: 0.3
    max_tokens: 4096
    timeout_ms: 180000  # 3 minutes

  # JSON Processing Model (clustering, formatting, validation)
  # Good at producing structured, valid JSON output
  json_model:
    provider: local
    model: ministral-3:latest
    temperature: 0.1  # Lower temp for consistent JSON
    max_tokens: 8192  # Larger context for clustering
    timeout_ms: 300000  # 5 minutes for complex tasks

  # Checkpoint configuration
  checkpoints:
    enabled: true
    output_dir: _bmad-output/secretary/checkpoints

# Alternative configurations for different use cases
alternatives:
  # Use Anthropic Claude for best quality
  anthropic:
    text_model:
      provider: anthropic
      model: claude-sonnet-4-20250514
      temperature: 0.3
      max_tokens: 8192
      timeout_ms: 180000
    json_model:
      provider: anthropic
      model: claude-sonnet-4-20250514
      temperature: 0.1
      max_tokens: 8192
      timeout_ms: 180000

  # Use OpenAI GPT-4o for faster processing
  openai:
    text_model:
      provider: openai
      model: gpt-4o
      temperature: 0.3
      max_tokens: 8192
      timeout_ms: 120000
    json_model:
      provider: openai
      model: gpt-4o
      temperature: 0.1
      max_tokens: 8192
      timeout_ms: 120000
